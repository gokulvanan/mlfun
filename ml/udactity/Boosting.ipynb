{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#http://machinelearningmastery.com/boosting-and-adaboost-for-machine-learning/\n",
    "#https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/\n",
    "#http://scikit-learn.org/stable/modules/ensemble.html#b1999\n",
    "\n",
    "Types Of Boosting:\n",
    "AdaBoosting (Adaptive Boosting) - works on base ML that understand weigthed training inputs eg DecisionTree and NaiveBayes\n",
    "\n",
    "AdaBoost is best used to boost the performance of decision trees on binary classification problems\n",
    "\n",
    "AdaBoost.M1 - used only for classification and not regression hence called discrete AdaBoost\n",
    "\n",
    "AdaBoost is best used with weak learners. learners with error rate > 50% (greader than chance)\n",
    "\n",
    "Most common use case - decision trees with one level (These trees being too short and containing one decision are often call decision stumps)\n",
    "\n",
    "Each training data is weighted. Initial weight per data is 1/n (n = number of data in dataset).\n",
    "\n",
    "The process involves building many decison stumps each with a weight based on its error rate. (The number of decision stumps based on if all the data points are being classified correctly or max number of decision stumps is eached.\n",
    "\n",
    "At each step the error rate is checked. resulting in weight of the decision stump being compute and updating weight of the older data to a higher value so that its considered in the next decision stump.\n",
    "\n",
    "Final outcome is weighted average of prediction of all the data stumps, in binary classification is weigthed voting.\n",
    "\n",
    "Important to keep in mind that in AdaBoost\n",
    "Data whould not have any noise or outliers or corruption.\n",
    "As the algo will focus on predicting the outliers in sucessive steps of building weak learners. \n",
    "\n",
    "\n",
    "You can tune the parameters to optimize the performance of algorithms, Iâ€™ve mentioned below the key parameters for tuning:\n",
    "\n",
    "n_estimators: It controls the number of weak learners.\n",
    "learning_rate:Controls the contribution of weak learners in the final combination. There is a trade-off between learning_rate and n_estimators.\n",
    "base_estimators: It helps to specify different ML algorithm.\n",
    "\n",
    "Gradient Tree Boosting:\n",
    "\n",
    "continue from https://www.analyticsvidhya.com/blog/2015/09/complete-guide-boosting-methods/\n",
    "XGBoost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "iris = load_iris()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), # this defined the base_estimator which can be \n",
    "                         #configured to have minimal tree height to be a weak learner\n",
    "                         n_estimators=100, # number of weak learners at which boosting is terminated\n",
    "                         # if not already terminated because of sucessfull classification (value determines overfitting)\n",
    "                         learning_rate=0.5 # rate of learning (shrinks the contribution of each classifier by this value)\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of weak learners is controlled by the parameter n_estimators. The learning_rate parameter controls the contribution of the weak learners in the final combination. By default, weak learners are decision stumps. Different weak learners can be specified through the base_estimator parameter. The main parameters to tune to obtain good results are n_estimators and the complexity of the base estimators (e.g., its depth max_depth or minimum required number of samples at a leaf min_samples_leaf in case of decision trees)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
