{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More info:\n",
    "http://scikit-learn.org/stable/modules/ensemble.html#adaboost\n",
    "\n",
    "The basic classification of Ensemble Methods:\n",
    "\n",
    "1) Averaging Methods (Bagging)\n",
    "\n",
    "2) Boosting Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "References:\n",
    "\n",
    "https://www.quora.com/Whats-the-difference-between-boosting-and-bagging\n",
    "\n",
    "http://quantdare.com/2016/04/what-is-the-difference-between-bagging-and-boosting/\n",
    "\n",
    "http://quantdare.com/2016/02/dream-team-combining-classifiers-2/\n",
    "\n",
    "http://scikit-learn.org/stable/modules/ensemble.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both bagging and boosting are ensemble methods focused in improving over the output of single base calssifier.\n",
    "\n",
    "Bagging is taken from Bootstrap Averaging. The primary objective of this method is averaging over the outputs of N sample data run fit of N instances of the same classfier and using this ensemble to average the output.\n",
    "\n",
    "Bagging Steps:\n",
    "\n",
    "1) Take N random Samples of data with replacement (With or without is configurable and termed diffenetly for now lets assumed with replacement).\n",
    "\n",
    "2) For each sample train the classifier using clf.fix(xx,yy); (This can run in parallel.)\n",
    "\n",
    "3) Now when we need to test for xx_test data, run the xx_test data on all N classifiers formed by training on N samples.\n",
    "\n",
    "4) Average the output of N classifier (Either avearge in case of continuous data or Voting in case of discrete data).\n",
    "\n",
    "5) Final output is the avearge of the output from ensemble.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting steps:\n",
    "\n",
    "1) Start with first random sample from data and train the base classifer (clf.fit(x,y));\n",
    "\n",
    "2) If error of this classifier is > 50% repeat with new sample (Classifier has to do better than chance in some cases like AdaBoost). Based on error of classifier assign a weight to it.. lesser error has higher weight say w1 for classifier c1.\n",
    "\n",
    "3) Take the next sample (with or without replacement) + Add data which were missclassified in first classification run. (in case of replacment this is done by increasing probability of selection of error data points in no replacement this is done by just adding the errors before selecting the next sample).\n",
    "\n",
    "4) Repeat from step 1 for N attempts. (Note miscalssified cases are emphasizied for each subsequent classification).\n",
    "\n",
    "5) Now when we need to test for xx_test data , run the xx_test_data on all N classifier instances and take weighted average. (Weight w computed as in step 2).\n",
    "\n",
    "6) Final output is weighted average output from the ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note:\n",
    "\n",
    "Bagging reduced overfitting and works well the base estimator is complex and overfitts the data.\n",
    "\n",
    "Boosting reduces bias, it works well with less compex models (shallow trees). As this works of failed test cases for every subsequent attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
