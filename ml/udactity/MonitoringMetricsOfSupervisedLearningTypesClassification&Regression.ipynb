{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification and Regression\n",
    "These are defined based on type of output metric that is being predicted. If the predicted target data is a discrete, like yes/no, spam/ham etc... its classification else if its continuous like price of house, loss in insurance claim etc. It is regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning evaluation metrics based on classification or regression\n",
    "In classification metrics checked are \n",
    "\n",
    "#### Accuracy\n",
    "number of collectly identfied instance over all instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = [0, 2, 1, 3]\n",
    "y_true = [0, 1, 2, 3]\n",
    "scorePerc = accuracy_score(y_true,y_pred)\n",
    "print scorePerc\n",
    "accuracy_score(y_true,y_pred,normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shortcomings of accuracy:\n",
    "Accuracy predicts if result is match or not. But there are cases where accuracy is not what we are looking for.\n",
    "\n",
    "\n",
    "Eg in cancer prediction we may want it to proactively detect cancer even if it gets a few false positives.. hence accuracy might be low with false positives but we ensure it detects cancer\n",
    "Also in case of skweded distribution accuracy is not the best metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision  and Recall\n",
    "So In algo there are 3 outcomes Correct, FalsePositive, False Negative.\n",
    "Correct when predicted == actual state\n",
    "False positive = when predicted true but is false \n",
    "False negative = when predicted false but is true.. (missing this in burglar alarm case is more costly)\n",
    "\n",
    "Note above can extened beyond binary true and false for more than one classification like predicted A but not A\n",
    "\n",
    "Precision is how many prediction we got correct from total predictions, CorrectlyPredicted()/Total Predicted()\n",
    "Recall is how many predictions we got correct from total actuals, CorrectlyPredicted()/ Total Correct Actuals();\n",
    "\n",
    "Recall takes into account how many misses (False negatives)\n",
    "Precision takes into account False Positives how correct we are.\n",
    "\n",
    "Some times we need better recall than Precision and other times vicecersa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.   0.5]\n",
      "[ 0.5  0.5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.5,  0.5])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "y_pred = [0, 1, 1, 3]\n",
    "y_true = [0, 1, 0, 1]\n",
    "precScore = precision_score(y_true,y_pred,average=None,labels=[0,1])\n",
    "print precScore  # 0th element is 1 as we predicted 0 cocrreclty out of total pred = 1\n",
    "#1st element is 0.5 as we predicted 1 correclty once and incorreclty second time.. False positive\n",
    "recallScore = recall_score(y_true,y_pred,average=None, labels=[0,1])\n",
    "print recallScore\n",
    "# both elemnet are 0.5 as total count of 0 and an 1 is 2 and we predicted correclty only once.. \n",
    "a_true = ['spam','ham','spam','ham']\n",
    "a_pred = ['spam','spam','ham','ham']\n",
    "recall_score(a_true,a_pred,average=None, labels=['spam','ham'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(a_true,a_pred,labels=['spam','ham']) # illustrates the confusion matrix from which we compute precision \n",
    "#and recall scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1Score\n",
    "The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal.\n",
    "\n",
    "F1 = 2 *(precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.66666667,  0.5       ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_true,y_pred,average=None,labels=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regression metrics checked are \n",
    "As mentioned earlier for regression type of problems we are dealing with model that make predictions on continuous data. In this case we care more about how close the prediction is.\n",
    "For example with height & weight predictions we do not care as much if the model can 100% accurately predict someone's weight down to a less than a fraction of the pound, but perhaps how consistently the model can make a close prediction (perhaps with 3-4 pounds of the individual).\n",
    "\n",
    "#### Mean Absolute Error\n",
    "The mean absolute error takes the total absolute error of each example and averages the error based on the number of data points.\n",
    "\n",
    "#### Mean squared error\n",
    "Some benefits of squaring the residual error is that it automatically converts all the errors as positives, emphasizes larger errors rather than smaller errors, and from calculus is differentiable which allows us to find the minimum or maximum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, median_absolute_error, mean_squared_error\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "abs_error = mean_absolute_error(y_true,y_pred)\n",
    "print abs_error\n",
    "print median_absolute_error(y_true,y_pred)\n",
    "mean_squared_error(y_true,y_pred)\n",
    "# not multioutput args can be used when input a composite data array, check mean_absolute_error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R2 Score (Corefficient of Determination)\n",
    "In statistics, the coefficient of determination, denoted R2 or r2 and pronounced \"R squared\", is a number that indicates the proportion of the variance in the dependent variable that is predictable from the independent variable.\n",
    "R2 would range from 1 to -infinity.. closer to 1 the better the prediction\n",
    "\n",
    "check https://en.wikipedia.org/wiki/Coefficient_of_determination for excelent description of R2.\n",
    "\n",
    "In breif \n",
    "R2 = 1 - (SSres/SStot) \n",
    "\n",
    "SStot = variance (sum of square of mean of true values - each true value)\n",
    "\n",
    "SSres = sums of squres of residue i.e. pred-actual  \n",
    "\n",
    "In high level R2 score takes into accout of spread of data using SStot and checks the spread of data with respect to predicted data (residue)SSres in proportion to its total => SSres/SStot.\n",
    "\n",
    "The lower the values of SSres/SStot the more accurate our prediction fits the real value irrespective of existing variance in the data.-- This is the difference from mean sqaure error, where initial variance of data is not considered in evaluating the prediction. Hence r2 score might give better value when our prediction is good but data is spread out hence prediction mean sqaured error is hight.\n",
    "\n",
    "Note: R2 can be negative when prediction is very bad and actual varaince is low such that SSres/SStot > 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94860813704496794"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_true,y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
