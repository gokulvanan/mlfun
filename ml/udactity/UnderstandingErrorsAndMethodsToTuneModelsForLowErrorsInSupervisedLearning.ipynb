{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causes of Error:\n",
    "\n",
    "### Bias (Underfitting)\n",
    "As mentioned before, bias occurs when a model has enough data but is not complex enough to capture the underlying relationships. As a result, the model consistently and systematically misrepresents the data, leading to low accuracy in prediction. This is known as underfitting.\n",
    "Simply put, bias occurs when we have an inadequate model. An example might be when we have objects that are classified by color and shape, but our model can only partition and classify objects just by color (it is an overly simplified model) and therefore consistently mislabels future objects.\n",
    "Or perhaps we have continuous data that is polynomial in nature but our model can only represent linear relationships. In this case it does not matter how much data we feed the model because it just cannot represent the underlying relationship that we need a more complex model for.\n",
    "\n",
    "\n",
    "### Variance (Overfitting)\n",
    "When we train a model, we typically use a limited number of samples from a larger population (the training set). If we repeatedly train a model with randomly selected subsets of data, we would expect its predictons to be different based on the specific examples given to it. Here variance is a measure of how much the predictions vary for any given test sample.\n",
    "Some variance is normal, but too much variance indicates that the model is unable to generalize its predictions to the larger population from which training samples were drawn. High sensitivity to the training set is also known as overfitting, and generally occurs when either the model is too complex and/or we do not have enough data to support it.\n",
    "We can typically reduce the variability of a model's predictions and increase precision by training on more data.\n",
    "\n",
    "Balancing bias and variance and getting the right spot with high r2 score is the trick in ML algo\n",
    "\n",
    "A good read on bias and variance:\n",
    "http://scott.fortmann-roe.com/docs/BiasVariance.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods to split data for being able to get the best model with low errors\n",
    "The best way to find the most optimal model is using guess and check. i.e. try out all possible models.\n",
    "Each model is built out of training data and then we rank this model by checking its error/score metric against test data.\n",
    "\n",
    "The way we do this is to split our smaple data in to train/test split which can be applied to the model and then evaludated\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "# how we now build x_train, y_train and x_test and y_test is explained below\n",
    "#clf.fit(x_train,y_train)\n",
    "#clf.score(x_test,y_test) #todo check how score function was injected to classfier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "Given a sample of data with expected result we would need to train on some parts of this and use some parts to test our ML model. This train/ test split from our sample can be done intutively using CrossValidaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n",
      "(90, 4) (60, 4) (90,) (60,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn import cross_validation, datasets\n",
    "iris = datasets.load_iris()\n",
    "print iris.data.shape, iris.target.shape # 150 rows in input data has 4 dimensions\n",
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(iris.data,iris.target, test_size=0.4, random_state=0)\n",
    "print x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### KFold\n",
    "Extending the basic cross validation is Kfold.\n",
    "Here the idea is to run cross validation K times i.e. split sample data in K folds and choose first fold as test data for first run  then the second fold and so on till the Kth fold.\n",
    "This way all data is used in training and testing and out of the K models we created with fitting training data pick the one with lowest error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3] [0 1]\n",
      "[0 1] [2 3]\n",
      "[[ 5.1  3.5  1.4  0.2]\n",
      " [ 4.9  3.   1.4  0.2]]\n",
      "[0 1 3 4 5 7 9] [2 6 8]\n",
      "[2 3 5 6 7 8 9] [0 1 4]\n",
      "[0 1 2 3 4 6 8 9] [5 7]\n",
      "[0 1 2 4 5 6 7 8] [3 9]\n"
     ]
    }
   ],
   "source": [
    "kf = cross_validation.KFold(4,n_folds=2) # total size of data and number of folds we want to make\n",
    "for train, test in kf:\n",
    "    print (\"%s %s\" % (train,test)) # will print 2 rows for 2 folds with sets swapped\n",
    "    \n",
    "# now this index can be used to pick train and corresponding test data\n",
    "#not index could be used like array index in input data eg..\n",
    "print iris.data[0:2] # returns 2 rows of iris data in index 0 and 2\n",
    "\n",
    "# hence for each run we do\n",
    "#build x_train, y_train, x_test, y_test from index\n",
    "#clf.fit(x_train,y_train)\n",
    "#clf.score(x_test,y_test)\n",
    "# pick the best clasffier\n",
    "    \n",
    "kf = cross_validation.KFold(10,n_folds=4,shuffle=True) # shuffle import to shuffle indices in data if first half \n",
    "# of data is  in one pattern and second is another pattern\n",
    "for train, test in kf:\n",
    "    print (\"%s %s\" % (train,test))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridCV\n",
    "Above is a lot of work.. Also for each combination of train_test we sometimes have need to run it against different parameters of our ML algo.. Incase of SVM the SVM algo has paramaters such as \n",
    "kernel = linear/rbf \n",
    "C = number from 1 to +ve range\n",
    "\n",
    "Hence a wapper class which takes Algo + array parameters and generates all combination of paremeter to be used with algo and runs this with Kfold cross validation split on train_data and test_data is GridSearchCV\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98\n",
      "{'kernel': 'linear', 'C': 1}\n",
      "make_scorer(accuracy_score)\n",
      "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "0.979947931698\n",
      "0.98\n",
      "0.973333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn import grid_search \n",
    "\n",
    "paramaters = { 'kernel':('linear','rbf'), 'C':[1,10] }  #params with all possibile options C range 1 to 10 \n",
    "                                                        #and kernel can be linear or rb\n",
    "\n",
    "svr = svm.SVC() #choose estimator\n",
    "\n",
    "\n",
    "clf = grid_search.GridSearchCV(svr,paramaters,scoring='accuracy')  # build grid search to combine esimtator with various combinations of \n",
    "                                                # paramteres to be build different classfiers\n",
    "clf.fit(iris.data,iris.target); # will carry out kfold cross validat\n",
    "\n",
    "print clf.best_score_\n",
    "print clf.best_params_\n",
    "print clf.scorer_\n",
    "\n",
    "print clf.best_estimator_\n",
    "\n",
    "clf = grid_search.GridSearchCV(svr,paramaters,scoring='f1_macro')  # build grid search to combine esimtator with various combinations of \n",
    "                                                # paramteres to be build different classfiers\n",
    "clf.fit(iris.data,iris.target); # will carry out kfold cross validat\n",
    "print clf.best_score_\n",
    "# scoring function can be customized using stirng that maps to function give in link\n",
    "# 'http://scikit-learn.org/stable/modules/model_evaluation.html'\n",
    "\n",
    "#Since clf is doing many combinations of ML model with params running. \n",
    "# It can run them parallely \n",
    "# parallelism hint is given by argument n_jobs\n",
    "\n",
    "clf = grid_search.GridSearchCV(svr,paramaters,scoring='accuracy',n_jobs=10)  \n",
    "clf.fit(iris.data,iris.target); \n",
    "\n",
    "print clf.best_score_\n",
    "\n",
    "# we can also pass numer of cross folds else by default it dtermines the cross fold stragey\n",
    "# check  http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation on how ot choose strategy options\n",
    "k_fold = cross_validation.KFold(len(iris.target), n_folds=6, shuffle=True, random_state=0)\n",
    "clf = grid_search.GridSearchCV(svr,paramaters,scoring='accuracy',n_jobs=10,cv=k_fold)  \n",
    "clf.fit(iris.data,iris.target); \n",
    "\n",
    "print clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve\n",
    "A curve with number of training points increasing on xaxis and error score on yaxis;\n",
    "if we have training data x_train, y_train and test x_test, y_test\n",
    "than 2 cuvers can be made\n",
    "by taking sample  x_train[:s] - where s increments by step size in xaxis\n",
    "fitting x_train[:s] and y_train[:s] into the ML model. \n",
    "and then checking for errorscore against what we trained for i.e. x_train[:s] y_train[:s] \n",
    "and the test data x_test, y_test.\n",
    "Will result in 2 curves the training cuver and testing curve for incrmented values of s.\n",
    "train cuver would move from 0 error score to some upper value while test will move from high to low.\n",
    "The point where the cuvers converge is where the model fits best.\n",
    "\n",
    "If the convergence to high in the y axis of error score we can assume the parameter we used in model is underfitting. as with larger training data our error on the trained data itself is still very high.\n",
    "\n",
    "If the convergence is low in the yaxis but the gap between test and train curve is high we can attirbute this to overfitting. our model has overfit the data such that test error score are higher than train and the graphs covergence gap is hight.\n",
    "\n",
    "\n",
    "\n",
    "### Model Complexity\n",
    "A curve with ML model parameter of increasing complexity (such as increasize max_dept in case of decision_tree) in xaxis and error score in yaxis is the complexity graph.\n",
    "\n",
    "Note here the error score is computed from same test data and model is trained on fixed size training data.\n",
    "The graph helps to clear identify OCams razor of overfitting /underfitting.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
