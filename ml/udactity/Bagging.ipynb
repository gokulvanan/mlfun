{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Meta Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Two types BaggingClassifier and BaggingRegressor\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "bagging = BaggingClassifier(base_estimator=KNeighborsClassifier(),\n",
    "                            n_estimators= 20, #number of estimators to be used in this ensemble\n",
    "                            max_samples=0.5,  #number of samples to draw from X to train each base estimator\n",
    "                            max_features=0.5, #number of features to draw from X to train each base estimator\n",
    "                            bootstrap=True, # if True samples are drawn with replacement\n",
    "                            bootstrap_features=True, # if true features are drawn with replacement\n",
    "                            oob_score=True,  # if true use out of bag samples to estimate\n",
    "                            n_jobs=1, # if -1 runs number of jobs as number of cores\n",
    "                            random_state=None # return what to use random samples\n",
    "                           ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forests of randomized trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random Forest RandomForestClassifier / RandomForestRegressor specialized ensemble specific for decisionTrees\n",
    "# has config opts of DecisionTreesClassifier and and BaggingClassifier in it\n",
    "# each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) \n",
    "# from the training set. In addition, when splitting a node during the construction of the tree, the split \n",
    "# that is chosen is no longer the best split among all features. Instead, the split that is picked is the best \n",
    "# split among a random subset of the features.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X = [[0,0],[1,1]]\n",
    "Y = [0,1]\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "print clf.fit(X,Y)\n",
    "\n",
    "\n",
    "#ExtraTrees\n",
    "# As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative \n",
    "# thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated \n",
    "# thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more,\n",
    "# at the expense of a slightly greater increase in bias\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf = ExtraTreesClassifier(n_estimators=10)\n",
    "clf.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main params are n_estimators and max_features\n",
    "\n",
    "n_estimators larger the better but more time to compute also note that results will stop getting  \n",
    "\n",
    "significantly better beyond a critical number of trees\n",
    "\n",
    "max_features = the size of the random subsets of features to consider when splitting a node.\n",
    "\n",
    "The lower the greater the reduction of variance, but also the greater the increase in bias\n",
    "\n",
    "Empirical good default values are:\n",
    "\n",
    "    max_features=n_features for regression problems, \n",
    "    max_features=sqrt(n_features) for classification tasks (where n_features is the number of features in the data).\n",
    "    max_depth=None in combination with min_samples_split=1 (full develop trees settings)\n",
    "    \n",
    "Note: The best parameter values should always be cross-validated. \n",
    "\n",
    "n_jobs = enables parallelization of computation\n",
    "\n",
    "The relative rank (i.e. depth) of a feature used as a decision node in a tree can be used to assess the relative importance of that feature with respect to the predictability of the target variable. Features used at the top of the tree contribute to the final prediction decision of a larger fraction of the input samples. The expected fraction of the samples they contribute to can thus be used as an estimate of the relative importance of the features.\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one of the outputs consumed from ExtraTreesClassifier is feature_importance\n",
    "\n",
    "To Continue from \n",
    "\n",
    "http://scikit-learn.org/stable/modules/ensemble.html#b1999\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_iris.html#sphx-glr-auto-examples-ensemble-plot-forest-iris-py\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances_faces.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-faces-py\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/plot_multioutput_face_completion.html#sphx-glr-auto-examples-plot-multioutput-face-completion-py\n",
    "\n",
    "\n",
    "Feature importance\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-py\n",
    "\n",
    "RamdonTreesEmbedding - Feature Transformation\n",
    "http://scikit-learn.org/stable/auto_examples/ensemble/plot_random_forest_embedding.html#sphx-glr-auto-examples-ensemble-plot-random-forest-embedding-py\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
